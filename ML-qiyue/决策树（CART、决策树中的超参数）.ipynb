{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"微软雅黑\" color=\"blue\" size=3>决策树（CART、决策树中的超参数）</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 老师：<font face= \"微软雅黑\" color=\"red\" size=1 >非参数学习的算法都容易产生过拟合</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face= \"微软雅黑\" color=\"black\" size=2 >**一、决策树模型的创建方式、时间复杂度**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 1）创建方式  \n",
    "1. 既可以解决分类问题，又可以解决回归问题；\n",
    "2. CART 创建决策树的方式：根据某一维度 d 和某一个 阈值 v 进行二分；（得到的是一个二叉树）\n",
    "3. scikit-learn 中的创建决策树的方式：CART（Classification And Regression Tree），也就是二叉树的方式；\n",
    "4. 创建决策树的方式有多种：ID3、C4.5、C5.0、CART；\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2）二叉树的实际复杂度;  \n",
    "   1.  预测样本时的时间复杂度：<font face= \"微软雅黑\" color=\"red\" size=1 >O(logm)</font> \n",
    "   2.  m：待预测的样本个数；  \n",
    "   3.  logm：表示“树”的高度，也就是 “树” 一共有 logm 层；  \n",
    "       可以这样理解：每一次的节点划分，都是对当前实际的对半划分，则“树”的高度是 logm ；当传入一个未知的新的样本时，从根节点处开始一步一步做决策，走到“叶子”节点；\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 训练决策树模型时的时间复杂度：<font face= \"微软雅黑\" color=\"red\" size=1 >O(n * m * logm)</font> \n",
    "  1.  X_train 是(m，n)   \n",
    "  2. “树”是 logm 级别的，如果每一层只划分一个节点，在一个节点上大概都要对数据集做 n*m 次划分，则训练的时间复杂度为 O(n*m*logm)；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<font face= \"微软雅黑\" color=\"black\" size=2 >**二、scikit-learn 中决策树算法的参数**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face= \"微软雅黑\" color=\"black\" size=1 >**1）决策树算法的问题**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练决策树模型的时间复杂度比较大：O(n* m* logm)；  \n",
    "训练决策树模型时容易产生过拟合；（这也是所有非参数学习算法的共同问题）  \n",
    "方案：剪枝——可降低复杂度，同时解决过拟合；  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " <font face= \"微软雅黑\" color=\"black\" size=1 >**2）常用参数**</font>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 参数的功能：<font size=1 color=\"red\" face=\"微软雅黑\">防止模型过拟合或欠拟合，降低训练模型的时间复杂度；</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt_clf = DecisionTreeClassifier(max_depth=2, criterion='entropy',\n",
    "                                                min_samples_split=10, min_samples_leaf=6, \n",
    "                                                max_leaf_nodes=4)\n",
    "dt_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth = 2：决策树模型的最高深度为 2；\n",
    "criterion = 'entropy'：划分节点数据集时，采用信息熵为判断条件；\n",
    "criterion = 'gini'：划分节点数据集时，采用基尼系数为判断条件；\n",
    "min_samples_split = 10：节点数据集中至少要有 10 个样本，才能继续对节点数据集进行划分,\n",
    "                                         否则即使信息熵和基尼系数都比较大，也不在划分数据集；\n",
    "min_samples_leaf = 6：一个“叶子”节点，至少要有 6 个样本；\n",
    "max_leaf_nodes = 4：模型最多只能有 4 个“叶子”节点；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"微软雅黑\" color=\"\t#00CED1\" size=2 >**机器学习：决策树（决策树解决回归问题、决策树算法的局限性）**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](决策树-3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: left;\" src=\"决策树-4.png\"  width=\"75%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = datasets.load_boston()\n",
    "X = boston.data\n",
    "y = boston.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5846731630478792"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=666)\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "dt_reg = DecisionTreeRegressor()\n",
    "dt_reg.fit(X_train,y_train)\n",
    "\n",
    "dt_reg.score(X_test,y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_reg.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **分析**：模型对训练数据集的预测准确度：100%，但在测试数据集上的准确度：58%，模型过拟合；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    模型在训练数据集上的表现非常好；  \n",
    "    模型在测试数据集上的表现不好"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"微软雅黑\" color=\"black\" size=1 >**二、决策树算法的局限性**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](决策树-5.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
